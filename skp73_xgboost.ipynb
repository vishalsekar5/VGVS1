{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ini=pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df_ini.columns[:]:\n",
    "    if df_ini[feature].dtype!='object':        \n",
    "        df_ini[feature]=np.where(df_ini[feature].isnull(),df_ini[feature].median(),df_ini[feature])\n",
    "    else:\n",
    "        df_ini[feature]=np.where(df_ini[feature].isnull(),df_ini[feature].value_counts().index[0],df_ini[feature])\n",
    "df_aim=df_ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reol(feature1):\n",
    "    IQR=df_aim[feature1].quantile(0.75)-df_aim[feature1].quantile(0.25)\n",
    "    lbound=df_aim[feature1].quantile(0.25)-(IQR*1.5)\n",
    "    ubound=df_aim[feature1].quantile(0.75)+(IQR*1.5)\n",
    "    df_aim.loc[df_aim[feature1]<=lbound,feature1]=lbound\n",
    "    df_aim.loc[df_aim[feature1]>=ubound,feature1]=ubound\n",
    "df_aol=df_aim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df_aim.columns[:]:\n",
    "    if df_aim[feature].dtype!='object':\n",
    "        reol(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aol_cont1=df_aol.select_dtypes(exclude=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aol_cont2=df_aol_cont1.drop(columns='Pclass',axis=1) #Dropping categorical ordinal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aol_cont3=df_aol_cont2.drop('Survived',axis=1)#varience threshold can be applied for independent features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarianceThreshold(threshold=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_thres=VarianceThreshold(threshold=0)\n",
    "var_thres.fit(df_aol_cont3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_features=[column for column in df_aol_cont3.columns\\\n",
    "               if column not in df_aol_cont3.columns[var_thres.get_support()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aol=df_aol.drop(const_features,axis=1)#Dropping constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aol_cont3=df_aol_cont3.drop(columns=const_features,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_aol_cont3\n",
    "y=df_aol_cont2['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Features to be Dropped are...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pcc(dataset,threshold):\n",
    "    dropset=set()\n",
    "    cor_mat=dataset.corr()\n",
    "    for i in range(len(cor_mat.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(cor_mat.iloc[i,j])>=threshold:\n",
    "                cor_feat=cor_mat.columns[i]\n",
    "                dropset.add(cor_feat)\n",
    "    return dropset \n",
    "dropset1=pcc(X,0.85)\n",
    "print(\"The Features to be Dropped are...\")\n",
    "dropset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aol=df_aol.drop(columns=dropset1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc(feature1):\n",
    "    key_labels=df_aol[feature1].unique()\n",
    "    dict={k:i for i,k in enumerate(key_labels,0)}\n",
    "    df_aol[feature1]=df_aol[feature1].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df_aol.columns[:]:\n",
    "    if df_aol[feature].dtype=='object':\n",
    "        enc(feature)\n",
    "df_aec=df_aol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "X=df_aec.drop(columns='Survived',axis=1)\n",
    "y=df_aec['Survived']\n",
    "mut_inf_data=mutual_info_classif(X,y,random_state=0)\n",
    "mut_inf_modif=pd.Series(mut_inf_data,index=X.columns)\n",
    "mut_inf_sorted=mut_inf_modif.sort_values(ascending=False)\n",
    "dict=mut_inf_sorted.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "droplist=[]\n",
    "for key in dict:\n",
    "    if dict[key]<0.001:\n",
    "        droplist.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aec=df_aec.drop(columns=droplist,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aim_cate1=df_aim.select_dtypes(include=[object])#For imbalanced data check,include all categorical features+Dep. feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aim_cate2=df_aim[['Pclass','Survived']].copy()#include categorical ordinal feature and\n",
    "df_aim_cate=pd.concat([df_aim_cate1,df_aim_cate2],axis=1)#dependent feature in the new Data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_chk_list=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df_aim_cate.columns[:]:\n",
    "    length=int(len(df_aim_cate[feature].unique()))      \n",
    "    if length <= 5:\n",
    "        imb_chk_list.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list=[]#Add the independent features with single category, imbalanced data in this list(Difference of values<0.05 and >0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aec=df_aec.drop(drop_list,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_list=['Survived','Pclass','Sex','SibSp','Cabin','Embarked']\n",
    "log_list=['Fare']\n",
    "expo_list=[]\n",
    "sqroot_list=['PassengerId','Name','Ticket']\n",
    "boxcox_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in log_list:\n",
    "    df_aec[feature]=np.log1p(df_aec[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in sqroot_list:\n",
    "    df_aec[feature]=df_aec[feature]**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atr=df_aec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "scaled=scaler.fit_transform(df_atr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ast=pd.DataFrame(scaled,columns=df_atr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin=df_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_fin.drop(columns='Survived',axis=1)\n",
    "y=df_fin['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.7,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "classifier=xgboost.XGBClassifier()\n",
    "classifier.fit(X_train,y_train)\n",
    "prediction=classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[149  19]\n",
      " [ 30  70]]\n",
      "Accuracy Score : 0.8171641791044776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.89      0.86       168\n",
      "         1.0       0.79      0.70      0.74       100\n",
      "\n",
      "    accuracy                           0.82       268\n",
      "   macro avg       0.81      0.79      0.80       268\n",
      "weighted avg       0.82      0.82      0.81       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "print(confusion_matrix(y_test,prediction))\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test,prediction))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "classifier=xgboost.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'learning_rate':[0.05,0.10,0.15,0.20,0.25,0.30],\n",
    "            'max_depth':[3,4,5,6,8,10,12],\n",
    "            'min_child_weight':[1,3,5,7],\n",
    "            'gamma':[0.0,0.1,0.2,0.3,0.4],\n",
    "            'colsample_bytree':[0.3,0.4,0.5,0.7]}            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search=GridSearchCV(estimator=classifier,param_grid=param_grid,cv=10,\n",
    "                         verbose=2,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3360 candidates, totalling 33600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:   35.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:   53.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4893 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5824 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6837 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7930 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9105 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 10360 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 11697 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 13114 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 14613 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done 16192 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 17853 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 19594 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=-1)]: Done 21417 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=-1)]: Done 23320 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done 25305 tasks      | elapsed: 23.2min\n",
      "[Parallel(n_jobs=-1)]: Done 27370 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=-1)]: Done 29517 tasks      | elapsed: 27.8min\n",
      "[Parallel(n_jobs=-1)]: Done 31744 tasks      | elapsed: 30.1min\n",
      "[Parallel(n_jobs=-1)]: Done 33600 out of 33600 | elapsed: 32.1min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:17:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_job...\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.3, 0.4, 0.5, 0.7],\n",
       "                         'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "                         'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
       "                         'max_depth': [3, 4, 5, 6, 8, 10, 12],\n",
       "                         'min_child_weight': [1, 3, 5, 7]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, gamma=0.0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.05, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=7, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid=grid_search.best_estimator_\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=best_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[148  20]\n",
      " [ 31  69]]\n",
      "Accuracy Score : 0.8097014925373134\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.88      0.85       168\n",
      "         1.0       0.78      0.69      0.73       100\n",
      "\n",
      "    accuracy                           0.81       268\n",
      "   macro avg       0.80      0.79      0.79       268\n",
      "weighted avg       0.81      0.81      0.81       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "print(confusion_matrix(y_test,prediction))\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test,prediction))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install optuna\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def objective(trial):\n",
    "    learning_rate=trial.suggest_categorical('learning_rate',[0.05,0.10,0.15,0.20,0.25,0.30])\n",
    "    max_depth=trial.suggest_categorical('max_depth',[3,4,5,6,8,10,12])\n",
    "    min_child_weight=trial.suggest_categorical('min_child_weight',[1,3,5,7])\n",
    "    gamma=trial.suggest_categorical('gamma',[0.0,0.1,0.2,0.3,0.4])\n",
    "    colsample_bytree=trial.suggest_categorical('colsample_bytree',[0.3,0.4,0.5,0.7])\n",
    "    classifier=xgboost.XGBClassifier(learning_rate=learning_rate,max_depth=max_depth,min_child_weight=min_child_weight,\n",
    "               gamma=gamma,colsample_bytree=colsample_bytree)\n",
    "    score=cross_val_score(classifier,X_train,y_train,n_jobs=-1,cv=3).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-29 21:47:43,504]\u001b[0m A new study created in memory with name: no-name-d5413337-169e-43fd-8309-26e204686508\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:46,729]\u001b[0m Trial 0 finished with value: 0.7753081258516041 and parameters: {'learning_rate': 0.3, 'max_depth': 12, 'min_child_weight': 7, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 0 with value: 0.7753081258516041.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:48,868]\u001b[0m Trial 1 finished with value: 0.7704926916883439 and parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_child_weight': 3, 'gamma': 0.3, 'colsample_bytree': 0.4}. Best is trial 0 with value: 0.7753081258516041.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:49,030]\u001b[0m Trial 2 finished with value: 0.7833673974978322 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 1, 'gamma': 0.0, 'colsample_bytree': 0.4}. Best is trial 2 with value: 0.7833673974978322.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:49,202]\u001b[0m Trial 3 finished with value: 0.7576566951566952 and parameters: {'learning_rate': 0.3, 'max_depth': 4, 'min_child_weight': 1, 'gamma': 0.4, 'colsample_bytree': 0.5}. Best is trial 2 with value: 0.7833673974978322.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:49,424]\u001b[0m Trial 4 finished with value: 0.7657082249473554 and parameters: {'learning_rate': 0.1, 'max_depth': 12, 'min_child_weight': 3, 'gamma': 0.1, 'colsample_bytree': 0.3}. Best is trial 2 with value: 0.7833673974978322.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:49,580]\u001b[0m Trial 5 finished with value: 0.788136380527685 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 7, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 5 with value: 0.788136380527685.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:49,765]\u001b[0m Trial 6 finished with value: 0.7817261241174286 and parameters: {'learning_rate': 0.2, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 0.1, 'colsample_bytree': 0.5}. Best is trial 5 with value: 0.788136380527685.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:50,022]\u001b[0m Trial 7 finished with value: 0.7560386473429951 and parameters: {'learning_rate': 0.25, 'max_depth': 10, 'min_child_weight': 1, 'gamma': 0.3, 'colsample_bytree': 0.3}. Best is trial 5 with value: 0.788136380527685.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:50,192]\u001b[0m Trial 8 finished with value: 0.7785287377678682 and parameters: {'learning_rate': 0.15, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 0.2, 'colsample_bytree': 0.3}. Best is trial 5 with value: 0.788136380527685.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:50,398]\u001b[0m Trial 9 finished with value: 0.7833441719311285 and parameters: {'learning_rate': 0.3, 'max_depth': 10, 'min_child_weight': 7, 'gamma': 0.2, 'colsample_bytree': 0.4}. Best is trial 5 with value: 0.788136380527685.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:50,615]\u001b[0m Trial 10 finished with value: 0.7865338164251208 and parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 5 with value: 0.788136380527685.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:50,826]\u001b[0m Trial 11 finished with value: 0.7865338164251208 and parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 5 with value: 0.788136380527685.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:50,981]\u001b[0m Trial 12 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:51,146]\u001b[0m Trial 13 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:51,300]\u001b[0m Trial 14 finished with value: 0.7849622197448284 and parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:51,515]\u001b[0m Trial 15 finished with value: 0.7576334695899912 and parameters: {'learning_rate': 0.25, 'max_depth': 8, 'min_child_weight': 5, 'gamma': 0.4, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:51,686]\u001b[0m Trial 16 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:51,831]\u001b[0m Trial 17 finished with value: 0.7817416078285643 and parameters: {'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:51,989]\u001b[0m Trial 18 finished with value: 0.7817570915397002 and parameters: {'learning_rate': 0.15, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.4, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:52,165]\u001b[0m Trial 19 finished with value: 0.7801158181592965 and parameters: {'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 5, 'gamma': 0.3, 'colsample_bytree': 0.5}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:52,419]\u001b[0m Trial 20 finished with value: 0.7753081258516041 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'min_child_weight': 3, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:52,589]\u001b[0m Trial 21 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:52,751]\u001b[0m Trial 22 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:52,918]\u001b[0m Trial 23 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:53,093]\u001b[0m Trial 24 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:53,261]\u001b[0m Trial 25 finished with value: 0.7945698625046451 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.2, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:53,420]\u001b[0m Trial 26 finished with value: 0.7753081258516041 and parameters: {'learning_rate': 0.15, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:53,630]\u001b[0m Trial 27 finished with value: 0.7705004335439117 and parameters: {'learning_rate': 0.2, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:53,841]\u001b[0m Trial 28 finished with value: 0.775315867707172 and parameters: {'learning_rate': 0.1, 'max_depth': 8, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.5}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:54,036]\u001b[0m Trial 29 finished with value: 0.7704926916883439 and parameters: {'learning_rate': 0.25, 'max_depth': 12, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.4}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:54,237]\u001b[0m Trial 30 finished with value: 0.7801390437260003 and parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'colsample_bytree': 0.3}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:54,404]\u001b[0m Trial 31 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-29 21:47:54,572]\u001b[0m Trial 32 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:54,735]\u001b[0m Trial 33 finished with value: 0.7849312523225566 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 3, 'gamma': 0.3, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:54,935]\u001b[0m Trial 34 finished with value: 0.773713303604608 and parameters: {'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:55,102]\u001b[0m Trial 35 finished with value: 0.7817338659729964 and parameters: {'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.4}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:55,284]\u001b[0m Trial 36 finished with value: 0.7865570419918245 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 1, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:55,549]\u001b[0m Trial 37 finished with value: 0.7688746438746438 and parameters: {'learning_rate': 0.05, 'max_depth': 12, 'min_child_weight': 3, 'gamma': 0.4, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:55,703]\u001b[0m Trial 38 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:55,895]\u001b[0m Trial 39 finished with value: 0.7785209959123002 and parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 7, 'gamma': 0.1, 'colsample_bytree': 0.5}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:56,051]\u001b[0m Trial 40 finished with value: 0.77532360956274 and parameters: {'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.2, 'colsample_bytree': 0.3}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:56,212]\u001b[0m Trial 41 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:56,383]\u001b[0m Trial 42 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:56,559]\u001b[0m Trial 43 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:56,745]\u001b[0m Trial 44 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:56,937]\u001b[0m Trial 45 finished with value: 0.7849699616003964 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 1, 'gamma': 0.1, 'colsample_bytree': 0.4}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:57,144]\u001b[0m Trial 46 finished with value: 0.7785132540567323 and parameters: {'learning_rate': 0.25, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 0.3, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:57,368]\u001b[0m Trial 47 finished with value: 0.7801235600148644 and parameters: {'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:57,603]\u001b[0m Trial 48 finished with value: 0.7624411618976836 and parameters: {'learning_rate': 0.2, 'max_depth': 10, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:57,765]\u001b[0m Trial 49 finished with value: 0.773713303604608 and parameters: {'learning_rate': 0.15, 'max_depth': 5, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.3}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:57,961]\u001b[0m Trial 50 finished with value: 0.792975040257649 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.4, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:58,152]\u001b[0m Trial 51 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:58,324]\u001b[0m Trial 52 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:58,537]\u001b[0m Trial 53 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:58,717]\u001b[0m Trial 54 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:58,875]\u001b[0m Trial 55 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:59,149]\u001b[0m Trial 56 finished with value: 0.773690078037904 and parameters: {'learning_rate': 0.05, 'max_depth': 12, 'min_child_weight': 3, 'gamma': 0.2, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:59,310]\u001b[0m Trial 57 finished with value: 0.773713303604608 and parameters: {'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:59,471]\u001b[0m Trial 58 finished with value: 0.788136380527685 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 7, 'gamma': 0.0, 'colsample_bytree': 0.5}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:59,734]\u001b[0m Trial 59 finished with value: 0.7817183822618605 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:47:59,897]\u001b[0m Trial 60 finished with value: 0.7849622197448284 and parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:00,065]\u001b[0m Trial 61 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:00,232]\u001b[0m Trial 62 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:00,401]\u001b[0m Trial 63 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-29 21:48:00,570]\u001b[0m Trial 64 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:00,790]\u001b[0m Trial 65 finished with value: 0.7833209463644245 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:00,959]\u001b[0m Trial 66 finished with value: 0.7849389941781246 and parameters: {'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.3, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:01,161]\u001b[0m Trial 67 finished with value: 0.7656695156695156 and parameters: {'learning_rate': 0.15, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'colsample_bytree': 0.4}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:01,368]\u001b[0m Trial 68 finished with value: 0.7833286882199926 and parameters: {'learning_rate': 0.25, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:01,547]\u001b[0m Trial 69 finished with value: 0.7849467360336925 and parameters: {'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 5, 'gamma': 0.4, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:01,769]\u001b[0m Trial 70 finished with value: 0.7865493001362567 and parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.3}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:01,986]\u001b[0m Trial 71 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:02,151]\u001b[0m Trial 72 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:02,318]\u001b[0m Trial 73 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:02,479]\u001b[0m Trial 74 finished with value: 0.794577604360213 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:02,650]\u001b[0m Trial 75 finished with value: 0.7833286882199926 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 3, 'gamma': 0.2, 'colsample_bytree': 0.7}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:02,812]\u001b[0m Trial 76 finished with value: 0.7849312523225566 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'colsample_bytree': 0.5}. Best is trial 12 with value: 0.794577604360213.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:02,989]\u001b[0m Trial 77 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:03,153]\u001b[0m Trial 78 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:03,338]\u001b[0m Trial 79 finished with value: 0.7737287873157438 and parameters: {'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:03,610]\u001b[0m Trial 80 finished with value: 0.7753003839960363 and parameters: {'learning_rate': 0.1, 'max_depth': 8, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:03,782]\u001b[0m Trial 81 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:03,948]\u001b[0m Trial 82 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:04,117]\u001b[0m Trial 83 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:04,296]\u001b[0m Trial 84 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:04,525]\u001b[0m Trial 85 finished with value: 0.7833209463644245 and parameters: {'learning_rate': 0.05, 'max_depth': 12, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:04,683]\u001b[0m Trial 86 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:04,846]\u001b[0m Trial 87 finished with value: 0.789746686485817 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 7, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:05,015]\u001b[0m Trial 88 finished with value: 0.7865493001362567 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.4}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:05,213]\u001b[0m Trial 89 finished with value: 0.7753081258516041 and parameters: {'learning_rate': 0.25, 'max_depth': 3, 'min_child_weight': 1, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:05,407]\u001b[0m Trial 90 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:05,603]\u001b[0m Trial 91 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:05,796]\u001b[0m Trial 92 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:05,983]\u001b[0m Trial 93 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:06,179]\u001b[0m Trial 94 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:06,377]\u001b[0m Trial 95 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-29 21:48:06,616]\u001b[0m Trial 96 finished with value: 0.7704694661216399 and parameters: {'learning_rate': 0.15, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:06,792]\u001b[0m Trial 97 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:06,970]\u001b[0m Trial 98 finished with value: 0.775315867707172 and parameters: {'learning_rate': 0.2, 'max_depth': 5, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.3}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 21:48:07,133]\u001b[0m Trial 99 finished with value: 0.7961801684627772 and parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.0, 'colsample_bytree': 0.7}. Best is trial 77 with value: 0.7961801684627772.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study=optuna.create_study(direction='maximize')\n",
    "study.optimize(objective,n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "best=study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 5,\n",
       " 'gamma': 0.0,\n",
       " 'colsample_bytree': 0.7}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier=xgboost.XGBClassifier(learning_rate=best['learning_rate'],max_depth=best['max_depth'],\n",
    "           min_child_weight=best['min_child_weight'],gamma=best['gamma'],colsample_bytree=best['colsample_bytree'])\n",
    "classifier.fit(X_train,y_train)\n",
    "prediction=classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[148  20]\n",
      " [ 28  72]]\n",
      "Accuracy Score : 0.8208955223880597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.88      0.86       168\n",
      "         1.0       0.78      0.72      0.75       100\n",
      "\n",
      "    accuracy                           0.82       268\n",
      "   macro avg       0.81      0.80      0.81       268\n",
      "weighted avg       0.82      0.82      0.82       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "print(confusion_matrix(y_test,prediction))\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test,prediction))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hyperopt\n",
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "space={'learning_rate':hp.choice('learning_rate',[0.05,0.10,0.15,0.20,0.25,0.30]),\n",
    "       'max_depth':hp.choice('max_depth',[3,4,5,6,8,10,12]),\n",
    "       'min_child_weight':hp.choice('min_child_weight',[1,3,5,7]),\n",
    "       'gamma':hp.choice('gamma',[0.0,0.1,0.2,0.3,0.4]),\n",
    "       'colsample_bytree':hp.choice('colsample_bytree',[0.3,0.4,0.5,0.7])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    classifier=xgboost.XGBClassifier(learning_rate=space['learning_rate'],max_depth=space['max_depth'],\n",
    "               min_child_weight=space['min_child_weight'],gamma=space['gamma'],colsample_bytree=space['colsample_bytree'])\n",
    "    score=cross_val_score(classifier,X_train,y_train,n_jobs=-1,cv=5).mean()\n",
    "    return{'loss':-score,'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 80/80 [00:35<00:00,  2.23trial/s, best loss: -0.7945935483870968]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0,\n",
       " 'gamma': 1,\n",
       " 'learning_rate': 2,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 3}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials=Trials()\n",
    "best=fmin(fn=objective,space=space,algo=tpe.suggest,max_evals=80,trials=trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lerat={0:0.05,1:0.10,2:0.15,3:0.20,4:0.25,5:0.30}\n",
    "maxdep={0:3,1:4,2:5,3:6,4:8,5:10,6:12}\n",
    "minchwe={0:1,1:3,2:5,3:7}\n",
    "gam={0:0.0,1:0.1,2:0.2,3:0.3,4:0.4}\n",
    "colsambyt={0:0.3,1:0.4,2:0.5,3:0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:30:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier=xgboost.XGBClassifier(learning_rate=lerat[best['learning_rate']],\n",
    "            max_depth=maxdep[best['max_depth']],min_child_weight=minchwe[best['min_child_weight']],\n",
    "            gamma=gam[best['gamma']],colsample_bytree=colsambyt[best['colsample_bytree']])\n",
    "classifier.fit(X_train,y_train)\n",
    "prediction=classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143  25]\n",
      " [ 30  70]]\n",
      "Accuracy Score : 0.7947761194029851\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       168\n",
      "         1.0       0.74      0.70      0.72       100\n",
      "\n",
      "    accuracy                           0.79       268\n",
      "   macro avg       0.78      0.78      0.78       268\n",
      "weighted avg       0.79      0.79      0.79       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "print(confusion_matrix(y_test,prediction))\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test,prediction))\n",
    "print(classification_report(y_test,prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
